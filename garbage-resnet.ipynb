{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819d7dfa",
   "metadata": {
    "papermill": {
     "duration": 0.003942,
     "end_time": "2025-02-28T13:12:41.007095",
     "exception": false,
     "start_time": "2025-02-28T13:12:41.003153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model RESNET18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9598f6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:41.014619Z",
     "iopub.status.busy": "2025-02-28T13:12:41.014412Z",
     "iopub.status.idle": "2025-02-28T13:12:44.259631Z",
     "shell.execute_reply": "2025-02-28T13:12:44.258995Z"
    },
    "papermill": {
     "duration": 3.250631,
     "end_time": "2025-02-28T13:12:44.261233",
     "exception": false,
     "start_time": "2025-02-28T13:12:41.010602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        self.dropout_percentage = 0.5\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # BLOCK-1 (starting block) input=(224x224) output=(112x112)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "        # BLOCK-2 (1) input=(112x112) output = (56x56)\n",
    "        self.conv2_1_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_1_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_1_2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # BLOCK-2 (2)\n",
    "        self.conv2_2_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_2_2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # BLOCK-3 (1) input=(56x56) output = (28x28)\n",
    "        self.conv3_1_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm3_1_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_1_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_1_2 = nn.BatchNorm2d(128)\n",
    "        self.concat_adjust_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout3_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        # BLOCK-3 (2)\n",
    "        self.conv3_2_1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_2_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_2_2 = nn.BatchNorm2d(128)\n",
    "        self.dropout3_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # BLOCK-4 (1) input=(28x28) output = (14x14)\n",
    "        self.conv4_1_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm4_1_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_1_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_1_2 = nn.BatchNorm2d(256)\n",
    "        self.concat_adjust_4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout4_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        # BLOCK-4 (2)\n",
    "        self.conv4_2_1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_2_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_2_2 = nn.BatchNorm2d(256)\n",
    "        self.dropout4_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # BLOCK-5 (1) input=(14x14) output = (7x7)\n",
    "        self.conv5_1_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm5_1_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_1_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_1_2 = nn.BatchNorm2d(512)\n",
    "        self.concat_adjust_5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout5_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        # BLOCK-5 (2)\n",
    "        self.conv5_2_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_2_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_2_2 = nn.BatchNorm2d(512)\n",
    "        self.dropout5_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "\n",
    "        # Final Block input=(7x7)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(7, 7), stride=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=1 * 1 * 512, out_features=1000)\n",
    "        self.out = nn.Linear(in_features=1000, out_features=num_classes)\n",
    "        # END\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1 --> Starting block\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        op1 = self.maxpool1(x)\n",
    "\n",
    "        # block2 - 1\n",
    "        x = self.relu(self.batchnorm2_1_1(self.conv2_1_1(op1)))  # conv2_1\n",
    "        x = self.batchnorm2_1_2(self.conv2_1_2(x))  # conv2_1\n",
    "        x = self.dropout2_1(x)\n",
    "        # block2 - Adjust - No adjust in this layer as dimensions are already same\n",
    "        # block2 - Concatenate 1\n",
    "        op2_1 = self.relu(x + op1)\n",
    "        # block2 - 2\n",
    "        x = self.relu(self.batchnorm2_2_1(self.conv2_2_1(op2_1)))  # conv2_2\n",
    "        x = self.batchnorm2_2_2(self.conv2_2_2(x))  # conv2_2\n",
    "        x = self.dropout2_2(x)\n",
    "        # op - block2\n",
    "        op2 = self.relu(x + op2_1)\n",
    "\n",
    "        # block3 - 1[Convolution block]\n",
    "        x = self.relu(self.batchnorm3_1_1(self.conv3_1_1(op2)))  # conv3_1\n",
    "        x = self.batchnorm3_1_2(self.conv3_1_2(x))  # conv3_1\n",
    "        x = self.dropout3_1(x)\n",
    "        # block3 - Adjust\n",
    "        op2 = self.concat_adjust_3(op2)  # SKIP CONNECTION\n",
    "        # block3 - Concatenate 1\n",
    "        op3_1 = self.relu(x + op2)\n",
    "        # block3 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm3_2_1(self.conv3_2_1(op3_1)))  # conv3_2\n",
    "        x = self.batchnorm3_2_2(self.conv3_2_2(x))  # conv3_2\n",
    "        x = self.dropout3_2(x)\n",
    "        # op - block3\n",
    "        op3 = self.relu(x + op3_1)\n",
    "\n",
    "        # block4 - 1[Convolition block]\n",
    "        x = self.relu(self.batchnorm4_1_1(self.conv4_1_1(op3)))  # conv4_1\n",
    "        x = self.batchnorm4_1_2(self.conv4_1_2(x))  # conv4_1\n",
    "        x = self.dropout4_1(x)\n",
    "        # block4 - Adjust\n",
    "        op3 = self.concat_adjust_4(op3)  # SKIP CONNECTION\n",
    "        # block4 - Concatenate 1\n",
    "        op4_1 = self.relu(x + op3)\n",
    "        # block4 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm4_2_1(self.conv4_2_1(op4_1)))  # conv4_2\n",
    "        x = self.batchnorm4_2_2(self.conv4_2_2(x))  # conv4_2\n",
    "        x = self.dropout4_2(x)\n",
    "        # op - block4\n",
    "        op4 = self.relu(x + op4_1)\n",
    "\n",
    "        # block5 - 1[Convolution Block]\n",
    "        x = self.relu(self.batchnorm5_1_1(self.conv5_1_1(op4)))  # conv5_1\n",
    "        x = self.batchnorm5_1_2(self.conv5_1_2(x))  # conv5_1\n",
    "        x = self.dropout5_1(x)\n",
    "        # block5 - Adjust\n",
    "        op4 = self.concat_adjust_5(op4)  # SKIP CONNECTION\n",
    "        # block5 - Concatenate 1\n",
    "        op5_1 = self.relu(x + op4)\n",
    "        # block5 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm5_2_1(self.conv5_2_1(op5_1)))  # conv5_2\n",
    "        x = self.batchnorm5_2_1(self.conv5_2_1(x))  # conv5_2\n",
    "        x = self.dropout5_2(x)\n",
    "        # op - block5\n",
    "        op5 = self.relu(x + op5_1)\n",
    "\n",
    "        # FINAL BLOCK - classifier\n",
    "        x = self.avgpool(op5)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba2687",
   "metadata": {
    "papermill": {
     "duration": 0.003544,
     "end_time": "2025-02-28T13:12:44.268993",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.265449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Resnet18 trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743428a1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.277171Z",
     "iopub.status.busy": "2025-02-28T13:12:44.276713Z",
     "iopub.status.idle": "2025-02-28T13:12:44.283488Z",
     "shell.execute_reply": "2025-02-28T13:12:44.282548Z"
    },
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-02-28T13:12:44.284862",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.272680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Thanh Le  16 April 2024\n",
    "# How to train/fine-tune a pre-trained model on a custom dataset (i.e., transfer learning)\n",
    "# \"\"\"\n",
    "# import torch\n",
    "# from torch import nn, save, load\n",
    "# from tqdm import tqdm\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchmetrics.functional import accuracy\n",
    "# from torchvision.transforms import ToTensor, Resize\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Setup CUDA\n",
    "# def setup_cuda():\n",
    "#     # Setting seeds for reproducibility\n",
    "#     seed = 50\n",
    "#     torch.backends.cudnn.enabled = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# def train_model():\n",
    "#     \"\"\"\n",
    "#     Train the model over a single epoch\n",
    "#     :return: training loss and training accuracy\n",
    "#     \"\"\"\n",
    "#     train_loss = 0.0\n",
    "#     train_acc = 0.0\n",
    "#     model.train()\n",
    "\n",
    "#     for (img, label) in tqdm(train_loader, ncols=80, desc='Training'):\n",
    "#         # Get a batch\n",
    "#         img, label = img.to(device, dtype=torch.float), label.to(device, dtype=torch.long)\n",
    "\n",
    "#         # Set the gradients to zero before starting backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Perform a feed-forward pass\n",
    "#         logits = model(img)\n",
    "\n",
    "#         # Compute the batch loss\n",
    "#         loss = loss_fn(logits, label)\n",
    "\n",
    "#         # Compute gradient of the loss fn w.r.t the trainable weights\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update the trainable weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate the batch loss\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # Get the predictions to calculate the accuracy for every iteration. Remember to accumulate the accuracy\n",
    "#         prediction = logits.argmax(axis=1)\n",
    "#         train_acc += accuracy(prediction, label, task='multiclass', average='macro', num_classes=len(class_names)).item()\n",
    "\n",
    "#     return train_loss / len(train_loader), train_acc / len(train_loader)\n",
    "\n",
    "\n",
    "# def validate_model():\n",
    "#     \"\"\"\n",
    "#     Validate the model over a single epoch\n",
    "#     :return: validation loss and validation accuracy\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     valid_loss = 0.0\n",
    "#     val_acc = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for (img, label) in tqdm(val_loader, ncols=80, desc='Valid'):\n",
    "#             # Get a batch\n",
    "#             img, label = img.to(device, dtype=torch.float), label.to(device, dtype=torch.long)\n",
    "\n",
    "#             # Perform a feed-forward pass\n",
    "#             logits = model(img)\n",
    "\n",
    "#             # Compute the batch loss\n",
    "#             loss = loss_fn(logits, label)\n",
    "\n",
    "#             # Accumulate the batch loss\n",
    "#             valid_loss += loss.item()\n",
    "\n",
    "#             # Get the predictions to calculate the accuracy for every iteration. Remember to accumulate the accuracy\n",
    "#             prediction = logits.argmax(axis=1)\n",
    "#             val_acc += accuracy(prediction, label, task='multiclass', average='macro', num_classes=len(class_names)).item()\n",
    "\n",
    "#     return valid_loss / len(val_loader), val_acc / len(val_loader)\n",
    "\n",
    "\n",
    "# # Example plotting function\n",
    "\n",
    "# def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "#     epochs = range(1, len(train_losses) + 1)\n",
    "#     # Losses\n",
    "#     plt.figure(figsize=(15, 7))\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "#     plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training and Validation Loss')\n",
    "#     plt.legend()\n",
    "#     plt.yscale('log')  # Log scale can help for loss curves with large values\n",
    "\n",
    "#     # Accuracies\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.plot(epochs, train_accuracies, label='Training Accuracy', color='green')\n",
    "#     plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     # Save the figure to a file\n",
    "#     plt.savefig(\"trainplot.png\")  # You can change the file name and format (e.g., .png, .jpg, .pdf)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = setup_cuda()\n",
    "\n",
    "#     # 1. Load the dataset\n",
    "#     transform = transforms.Compose([Resize((224, 224)), ToTensor()])\n",
    "#     train_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/train', transform=transform)\n",
    "#     val_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/val', transform=transform)\n",
    "#     # Get class names\n",
    "#     class_names = train_dataset.classes\n",
    "\n",
    "#     # 2. Create data loaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#     # 3. Create a new deep model without pre-trained weights\n",
    "#     # from utils.seresnet18 import ResNet18\n",
    "#     model = ResNet18(\n",
    "#         num_classes=len(class_names),\n",
    "#     ).to(device)\n",
    "\n",
    "#     # 4. Specify loss function and optimizer\n",
    "#     optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     # 5. Train the model with 100 epochs\n",
    "#     # store the metrics for plotting\n",
    "#     train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "#     max_acc = 0\n",
    "#     for epoch in range(100):\n",
    "\n",
    "#         # 5.1. Train the model over a single epoch\n",
    "#         train_loss, train_acc = train_model()\n",
    "#         train_losses.append(train_loss)     # save train loss values\n",
    "#         train_accuracies.append(train_acc)  # save train acc values\n",
    "\n",
    "#         # 5.2. Validate the model after training\n",
    "#         val_loss, val_acc = validate_model()\n",
    "#         val_losses.append(val_loss)         # save val loss values\n",
    "#         val_accuracies.append(val_acc)      # save val acc values\n",
    "\n",
    "#         print(f'Epoch {epoch}: Train loss = {train_loss}, Train accuracy: {train_acc}')\n",
    "#         print(f'Epoch {epoch}: Validation loss = {val_loss}, Validation accuracy: {val_acc}')\n",
    "\n",
    "#         # 4.3. Save the model if the validation accuracy is increasing\n",
    "#         if val_acc > max_acc:\n",
    "#             print(f'Validation accuracy increased ({max_acc} --> {val_acc}). Model saved')\n",
    "#             folder_path = 'checkpoints_resnet18'  # Define the folder name\n",
    "#             if not os.path.exists(folder_path):\n",
    "#                 os.makedirs(folder_path)  # Create the folder if it does not exist\n",
    "#             file_path = os.path.join(folder_path,\n",
    "#                                      'resnet18_epoch_' + str(epoch) + '_acc_{0:.4f}'.format(val_acc) + '.pt')\n",
    "#             with open(file_path, 'wb') as f:\n",
    "#                 save(model.state_dict(), f)\n",
    "#             max_acc = val_acc\n",
    "\n",
    "# # After training is complete, plot the metrics\n",
    "# plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c5fca",
   "metadata": {
    "papermill": {
     "duration": 0.004075,
     "end_time": "2025-02-28T13:12:44.295235",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.291160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing for Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea6ec3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.307720Z",
     "iopub.status.busy": "2025-02-28T13:12:44.307367Z",
     "iopub.status.idle": "2025-02-28T13:12:44.314302Z",
     "shell.execute_reply": "2025-02-28T13:12:44.313423Z"
    },
    "papermill": {
     "duration": 0.015551,
     "end_time": "2025-02-28T13:12:44.316075",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.300524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn, save, load\n",
    "# from tqdm import tqdm\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchmetrics.functional import accuracy\n",
    "# from torchvision.transforms import ToTensor, Resize\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Thiết lập biến cần thiết\n",
    "# train_dir = '/kaggle/input/dataset-split/dataset_split/train'\n",
    "# test_dir = '/kaggle/input/dataset-split/dataset_split/test'\n",
    "# valid_dir = '/kaggle/input/dataset-split/dataset_split/val'\n",
    "# NUM_WORKERS = os.cpu_count()\n",
    "# BATCH_SIZE = 32\n",
    "# IMG_SIZE = 224\n",
    "# manual_transforms = transforms.Compose([\n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# patch_size = 16\n",
    "# CLASS = {} # KHONG CAN\n",
    "\n",
    "\n",
    "# # Thiết lập thiết bị (GPU hoặc CPU)\n",
    "# def setup_cuda():\n",
    "#     torch.backends.cudnn.enabled = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#     return device\n",
    "\n",
    "\n",
    "# device = setup_cuda()\n",
    "\n",
    "# # Thiết lập các transform để xử lý ảnh\n",
    "# manual_transforms = transforms.Compose([\n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Hàm dự đoán kết quả cho một hình ảnh\n",
    "# def predict_image(image_path, model, transform, class_names, device):\n",
    "#     model.eval()\n",
    "#     img = Image.open(image_path).convert('RGB')\n",
    "#     img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         output = model(img_tensor)\n",
    "#         _, predicted_class = torch.max(output, 1)\n",
    "#     predicted_label = class_names[predicted_class.item()]\n",
    "#     return img, predicted_label\n",
    "\n",
    "\n",
    "# # Hàm chính để dự đoán các hình ảnh trong tập test\n",
    "# def test_model():\n",
    "#     # 1. Tải dữ liệu và lớp từ tập train\n",
    "#     transform = transforms.Compose([Resize((224, 224)), ToTensor()])\n",
    "#     train_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/train', transform=transform)\n",
    "#     test_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/test', transform=transform)\n",
    "#     # Get class names\n",
    "#     class_names = train_dataset.classes\n",
    "\n",
    "#     # 2. Tạo mô hình ViT và tải trạng thái từ checkpoint\n",
    "#     # from utils.resnet18 import ResNet18\n",
    "\n",
    "#     model = ResNet18(\n",
    "#         num_classes=len(class_names),\n",
    "#     ).to(device)\n",
    "\n",
    "#     folder_checkpoint = 'checkpoints_resnet18'  # Define the folder name\n",
    "#     file_name = '/kaggle/input/resnet-train-acc/ResNet_train_acc/resnet18_epoch_10_acc_0.7310.pt' #best weight\n",
    "#     file_checkpoint = os.path.join(folder_checkpoint, file_name)  # lay best weight\n",
    "#     model.load_state_dict(torch.load(file_checkpoint, device))\n",
    "#     print('Model loaded from checkpoint.')\n",
    "#     # Ensure the output directory exists\n",
    "#     output_dir = \"output_resnet18\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # 3. Dự đoán kết quả cho mỗi hình ảnh trong tập test\n",
    "#     for image_path in tqdm(test_dataset.imgs, desc='Testing'):\n",
    "#         img, predicted_label = predict_image(image_path[0], model, manual_transforms, class_names, device)\n",
    "#         # plt.imshow(img)\n",
    "#         # plt.title(f'Predicted: {predicted_label}')\n",
    "#         # plt.show()\n",
    "\n",
    "#         # Convert the tensor image back to a PIL image if necessary\n",
    "#         if isinstance(img, torch.Tensor):\n",
    "#             img = transforms.ToPILImage()(img)\n",
    "\n",
    "#         # Create a plot\n",
    "#         fig, ax = plt.subplots()\n",
    "\n",
    "#         # Set white background\n",
    "#         fig.patch.set_facecolor('white')\n",
    "#         ax.set_facecolor('white')\n",
    "\n",
    "#         # Remove axis\n",
    "#         ax.axis('off')\n",
    "\n",
    "#         # Display the image\n",
    "#         ax.imshow(img)\n",
    "\n",
    "#         # Add the predicted label as the title\n",
    "#         ax.set_title(f'Predicted: {predicted_label}', fontsize=12, pad=10)\n",
    "\n",
    "#         # Save the figure\n",
    "#         image_basename = os.path.basename(image_path[0])\n",
    "#         image_name, image_ext = os.path.splitext(image_basename)\n",
    "#         output_image_path = os.path.join(output_dir, f\"{image_name}_pred_{predicted_label}.png\")\n",
    "\n",
    "#         plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0.1)\n",
    "#         plt.close(fig)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     test_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b5a72",
   "metadata": {
    "papermill": {
     "duration": 0.00407,
     "end_time": "2025-02-28T13:12:44.326432",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.322362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SEBLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f0a82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.337091Z",
     "iopub.status.busy": "2025-02-28T13:12:44.336785Z",
     "iopub.status.idle": "2025-02-28T13:12:44.343730Z",
     "shell.execute_reply": "2025-02-28T13:12:44.342766Z"
    },
    "papermill": {
     "duration": 0.015406,
     "end_time": "2025-02-28T13:12:44.345345",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.329939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, c, r=16):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c0148",
   "metadata": {
    "papermill": {
     "duration": 0.005133,
     "end_time": "2025-02-28T13:12:44.354368",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.349235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SEBLOCK + Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b4f682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.362014Z",
     "iopub.status.busy": "2025-02-28T13:12:44.361774Z",
     "iopub.status.idle": "2025-02-28T13:12:44.393497Z",
     "shell.execute_reply": "2025-02-28T13:12:44.392646Z"
    },
    "papermill": {
     "duration": 0.03748,
     "end_time": "2025-02-28T13:12:44.395304",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.357824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, input_channels, reduction_ratio=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(input_channels, input_channels // reduction_ratio, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(input_channels // reduction_ratio, input_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(batch_size, channels, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class SEResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SEResNet18, self).__init__()\n",
    "\n",
    "        self.dropout_percentage = 0.5\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # BLOCK-1 (starting block) input=(224x224) output=(56x56)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "        # BLOCK-2 (1) input=(56x56) output = (56x56)\n",
    "        self.conv2_1_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_1_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_1_2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se2_1 = SEBlock(64)\n",
    "\n",
    "        # BLOCK-2 (2)\n",
    "        self.conv2_2_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm2_2_2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se2_2 = SEBlock(64)\n",
    "\n",
    "        # BLOCK-3 (1) input=(56x56) output = (28x28)\n",
    "        self.conv3_1_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm3_1_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_1_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_1_2 = nn.BatchNorm2d(128)\n",
    "        self.concat_adjust_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout3_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se3_1 = SEBlock(128)\n",
    "\n",
    "        # BLOCK-3 (2)\n",
    "        self.conv3_2_1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_2_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm3_2_2 = nn.BatchNorm2d(128)\n",
    "        self.dropout3_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se3_2 = SEBlock(128)\n",
    "\n",
    "        # BLOCK-4 (1) input=(28x28) output = (14x14)\n",
    "        self.conv4_1_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm4_1_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_1_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_1_2 = nn.BatchNorm2d(256)\n",
    "        self.concat_adjust_4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout4_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se4_1 = SEBlock(256)\n",
    "\n",
    "        # BLOCK-4 (2)\n",
    "        self.conv4_2_1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_2_1 = nn.BatchNorm2d(256)\n",
    "        self.conv4_2_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm4_2_2 = nn.BatchNorm2d(256)\n",
    "        self.dropout4_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se4_2 = SEBlock(256)\n",
    "\n",
    "        # BLOCK-5 (1) input=(14x14) output = (7x7)\n",
    "        self.conv5_1_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.batchnorm5_1_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_1_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_1_2 = nn.BatchNorm2d(512)\n",
    "        self.concat_adjust_5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(1, 1), stride=(2, 2),\n",
    "                                         padding=(0, 0))\n",
    "        self.dropout5_1 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se5_1 = SEBlock(512)\n",
    "\n",
    "        # BLOCK-5 (2)\n",
    "        self.conv5_2_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_2_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.batchnorm5_2_2 = nn.BatchNorm2d(512)\n",
    "        self.dropout5_2 = nn.Dropout(p=self.dropout_percentage)\n",
    "        self.se5_2 = SEBlock(512)\n",
    "\n",
    "        # Final Block input=(7x7)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=(7, 7), stride=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=1 * 1 * 512, out_features=1000)\n",
    "        self.out = nn.Linear(in_features=1000, out_features=num_classes)\n",
    "        # END\n",
    "\n",
    "    def forward(self, x):\n",
    "        # block 1 --> Starting block\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        op1 = self.maxpool1(x)\n",
    "\n",
    "        # block2 - 1\n",
    "        x = self.relu(self.batchnorm2_1_1(self.conv2_1_1(op1)))  # conv2_1\n",
    "        x = self.batchnorm2_1_2(self.conv2_1_2(x))  # conv2_1\n",
    "        x = self.dropout2_1(x)\n",
    "        x = self.se2_1(x)\n",
    "        # block2 - Adjust - No adjust in this layer as dimensions are already same\n",
    "        # block2 - Concatenate 1\n",
    "        op2_1 = self.relu(x + op1)\n",
    "        # block2 - 2\n",
    "        x = self.relu(self.batchnorm2_2_1(self.conv2_2_1(op2_1)))  # conv2_2\n",
    "        x = self.batchnorm2_2_2(self.conv2_2_2(x))  # conv2_2\n",
    "        x = self.dropout2_2(x)\n",
    "        x = self.se2_2(x)\n",
    "        # op - block2\n",
    "        op2 = self.relu(x + op2_1)\n",
    "\n",
    "        # block3 - 1[Convolution block]\n",
    "        x = self.relu(self.batchnorm3_1_1(self.conv3_1_1(op2)))  # conv3_1\n",
    "        x = self.batchnorm3_1_2(self.conv3_1_2(x))  # conv3_1\n",
    "        x = self.dropout3_1(x)\n",
    "        x = self.se3_1(x)\n",
    "        # block3 - Adjust\n",
    "        op2 = self.concat_adjust_3(op2)  # SKIP CONNECTION\n",
    "        # block3 - Concatenate 1\n",
    "        op3_1 = self.relu(x + op2)\n",
    "        # block3 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm3_2_1(self.conv3_2_1(op3_1)))  # conv3_2\n",
    "        x = self.batchnorm3_2_2(self.conv3_2_2(x))  # conv3_2\n",
    "        x = self.dropout3_2(x)\n",
    "        x = self.se3_2(x)\n",
    "        # op - block3\n",
    "        op3 = self.relu(x + op3_1)\n",
    "\n",
    "        # block4 - 1[Convolition block]\n",
    "        x = self.relu(self.batchnorm4_1_1(self.conv4_1_1(op3)))  # conv4_1\n",
    "        x = self.batchnorm4_1_2(self.conv4_1_2(x))  # conv4_1\n",
    "        x = self.dropout4_1(x)\n",
    "        x = self.se4_1(x)\n",
    "        # block4 - Adjust\n",
    "        op3 = self.concat_adjust_4(op3)  # SKIP CONNECTION\n",
    "        # block4 - Concatenate 1\n",
    "        op4_1 = self.relu(x + op3)\n",
    "        # block4 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm4_2_1(self.conv4_2_1(op4_1)))  # conv4_2\n",
    "        x = self.batchnorm4_2_2(self.conv4_2_2(x))  # conv4_2\n",
    "        x = self.dropout4_2(x)\n",
    "        x = self.se4_2(x)\n",
    "        # op - block4\n",
    "        op4 = self.relu(x + op4_1)\n",
    "\n",
    "        # block5 - 1[Convolution Block]\n",
    "        x = self.relu(self.batchnorm5_1_1(self.conv5_1_1(op4)))  # conv5_1\n",
    "        x = self.batchnorm5_1_2(self.conv5_1_2(x))  # conv5_1\n",
    "        x = self.dropout5_1(x)\n",
    "        x = self.se5_1(x)\n",
    "        # block5 - Adjust\n",
    "        op4 = self.concat_adjust_5(op4)  # SKIP CONNECTION\n",
    "        # block5 - Concatenate 1\n",
    "        op5_1 = self.relu(x + op4)\n",
    "        # block5 - 2[Identity Block]\n",
    "        x = self.relu(self.batchnorm5_2_1(self.conv5_2_1(op5_1)))  # conv5_2\n",
    "        x = self.batchnorm5_2_2(self.conv5_2_1(x))  # conv5_2\n",
    "        x = self.dropout5_2(x)\n",
    "        x = self.se5_2(x)\n",
    "        # op - block5\n",
    "        op5 = self.relu(x + op5_1)\n",
    "\n",
    "        # FINAL BLOCK - classifier\n",
    "        x = self.avgpool(op5)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8349e",
   "metadata": {
    "papermill": {
     "duration": 0.004918,
     "end_time": "2025-02-28T13:12:44.404127",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.399209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train SEBLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36681c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.414103Z",
     "iopub.status.busy": "2025-02-28T13:12:44.413846Z",
     "iopub.status.idle": "2025-02-28T13:12:44.418498Z",
     "shell.execute_reply": "2025-02-28T13:12:44.417811Z"
    },
    "papermill": {
     "duration": 0.009901,
     "end_time": "2025-02-28T13:12:44.419660",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.409759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Thanh Le  16 April 2024\n",
    "# How to train/fine-tune a pre-trained model on a custom dataset (i.e., transfer learning)\n",
    "# \"\"\"\n",
    "# import torch\n",
    "# from torch import nn, save, load\n",
    "# from tqdm import tqdm\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchmetrics.functional import accuracy\n",
    "# from torchvision.transforms import ToTensor, Resize\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Setup CUDA\n",
    "# def setup_cuda():\n",
    "#     # Setting seeds for reproducibility\n",
    "#     seed = 50\n",
    "#     torch.backends.cudnn.enabled = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     return torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# def train_model():\n",
    "#     \"\"\"\n",
    "#     Train the model over a single epoch\n",
    "#     :return: training loss and training accuracy\n",
    "#     \"\"\"\n",
    "#     train_loss = 0.0\n",
    "#     train_acc = 0.0\n",
    "#     model.train()\n",
    "\n",
    "#     for (img, label) in tqdm(train_loader, ncols=80, desc='Training'):\n",
    "#         # Get a batch\n",
    "#         img, label = img.to(device, dtype=torch.float), label.to(device, dtype=torch.long)\n",
    "\n",
    "#         # Set the gradients to zero before starting backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Perform a feed-forward pass\n",
    "#         logits = model(img)\n",
    "\n",
    "#         # Compute the batch loss\n",
    "#         loss = loss_fn(logits, label)\n",
    "\n",
    "#         # Compute gradient of the loss fn w.r.t the trainable weights\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update the trainable weights\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate the batch loss\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # Get the predictions to calculate the accuracy for every iteration. Remember to accumulate the accuracy\n",
    "#         prediction = logits.argmax(axis=1)\n",
    "#         train_acc += accuracy(prediction, label, task='multiclass', average='macro', num_classes=len(class_names)).item()\n",
    "\n",
    "#     return train_loss / len(train_loader), train_acc / len(train_loader)\n",
    "\n",
    "\n",
    "# def validate_model():\n",
    "#     \"\"\"\n",
    "#     Validate the model over a single epoch\n",
    "#     :return: validation loss and validation accuracy\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     valid_loss = 0.0\n",
    "#     val_acc = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for (img, label) in tqdm(val_loader, ncols=80, desc='Valid'):\n",
    "#             # Get a batch\n",
    "#             img, label = img.to(device, dtype=torch.float), label.to(device, dtype=torch.long)\n",
    "\n",
    "#             # Perform a feed-forward pass\n",
    "#             logits = model(img)\n",
    "\n",
    "#             # Compute the batch loss\n",
    "#             loss = loss_fn(logits, label)\n",
    "\n",
    "#             # Accumulate the batch loss\n",
    "#             valid_loss += loss.item()\n",
    "\n",
    "#             # Get the predictions to calculate the accuracy for every iteration. Remember to accumulate the accuracy\n",
    "#             prediction = logits.argmax(axis=1)\n",
    "#             val_acc += accuracy(prediction, label, task='multiclass', average='macro', num_classes=len(class_names)).item()\n",
    "\n",
    "#     return valid_loss / len(val_loader), val_acc / len(val_loader)\n",
    "\n",
    "# # Example plotting function\n",
    "\n",
    "# def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "#     epochs = range(1, len(train_losses) + 1)\n",
    "#     # Losses\n",
    "#     plt.figure(figsize=(15, 7))\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "#     plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training and Validation Loss')\n",
    "#     plt.legend()\n",
    "#     plt.yscale('log')  # Log scale can help for loss curves with large values\n",
    "\n",
    "#     # Accuracies\n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.plot(epochs, train_accuracies, label='Training Accuracy', color='green')\n",
    "#     plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='orange')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     # Save the figure to a file\n",
    "#     plt.savefig(\"trainplot.png\")  # You can change the file name and format (e.g., .png, .jpg, .pdf)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     device = setup_cuda()\n",
    "\n",
    "#     # 1. Load the dataset\n",
    "#     transform = transforms.Compose([Resize((224, 224)), ToTensor()])\n",
    "#     train_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/train', transform=transform)\n",
    "#     val_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/val', transform=transform)\n",
    "#     # Get class names\n",
    "#     class_names = train_dataset.classes\n",
    "\n",
    "#     # 2. Create data loaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#     # 3. Create a new deep model without pre-trained weights\n",
    "#     # from utils.seresnet18 import SEResNet18\n",
    "\n",
    "#     model = SEResNet18(\n",
    "#         num_classes=len(class_names),\n",
    "#     ).to(device)\n",
    "\n",
    "#     # 4. Specify loss function and optimizer\n",
    "#     optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     # 5. Train the model with 100 epochs\n",
    "#     # store the metrics for plotting\n",
    "#     train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "#     max_acc = 0\n",
    "#     for epoch in range(100):\n",
    "\n",
    "#         # 5.1. Train the model over a single epoch\n",
    "#         train_loss, train_acc = train_model()\n",
    "#         train_losses.append(train_loss)  # save train loss values\n",
    "#         train_accuracies.append(train_acc)  # save train acc values\n",
    "\n",
    "#         # 5.2. Validate the model after training\n",
    "#         val_loss, val_acc = validate_model()\n",
    "#         val_losses.append(val_loss)  # save val loss values\n",
    "#         val_accuracies.append(val_acc)  # save val acc values\n",
    "\n",
    "#         print(f'Epoch {epoch}: Train loss = {train_loss}, Train accuracy: {train_acc}')\n",
    "#         print(f'Epoch {epoch}: Validation loss = {val_loss}, Validation accuracy: {val_acc}')\n",
    "\n",
    "#         # 4.3. Save the model if the validation accuracy is increasing\n",
    "#         if val_acc > max_acc:\n",
    "#             print(f'Validation accuracy increased ({max_acc} --> {val_acc}). Model saved')\n",
    "#             folder_path = 'checkpoints_se_resnet18'  # Define the folder name\n",
    "#             if not os.path.exists(folder_path):\n",
    "#                 os.makedirs(folder_path)  # Create the folder if it does not exist\n",
    "#             file_path = os.path.join(folder_path,\n",
    "#                                      'se_resnet18_epoch_' + str(epoch) + '_acc_{0:.4f}'.format(val_acc) + '.pt')\n",
    "#             with open(file_path, 'wb') as f:\n",
    "#                 save(model.state_dict(), f)\n",
    "#             max_acc = val_acc\n",
    "# # After training is complete, plot the metrics\n",
    "# plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdb879",
   "metadata": {
    "papermill": {
     "duration": 0.003052,
     "end_time": "2025-02-28T13:12:44.425929",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.422877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing SEResnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b672b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T13:12:44.433250Z",
     "iopub.status.busy": "2025-02-28T13:12:44.433011Z",
     "iopub.status.idle": "2025-02-28T13:40:12.255179Z",
     "shell.execute_reply": "2025-02-28T13:40:12.254111Z"
    },
    "papermill": {
     "duration": 1647.827451,
     "end_time": "2025-02-28T13:40:12.256651",
     "exception": false,
     "start_time": "2025-02-28T13:12:44.429200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a2785d20ffa9>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_checkpoint, device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  12%|█▏        | 998/8463 [02:56<30:33,  4.07it/s]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Testing: 100%|██████████| 8463/8463 [27:00<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, save, load\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Thiết lập biến cần thiết\n",
    "train_dir = '/kaggle/input/playcards/train'\n",
    "test_dir = '/kaggle/input/playcards/test'\n",
    "valid_dir = '/kaggle/input/playcards/valid'\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "patch_size = 16\n",
    "CLASS = {} # KHONG CAN DIEN CHI TIET\n",
    "\n",
    "\n",
    "# Thiết lập thiết bị (GPU hoặc CPU)\n",
    "def setup_cuda():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "device = setup_cuda()\n",
    "\n",
    "# Thiết lập các transform để xử lý ảnh\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Hàm dự đoán kết quả cho một hình ảnh\n",
    "def predict_image(image_path, model, transform, class_names, device):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    predicted_label = class_names[predicted_class.item()]\n",
    "    return img, predicted_label\n",
    "\n",
    "\n",
    "# Hàm chính để dự đoán các hình ảnh trong tập test\n",
    "def test_model():\n",
    "    # 1. Tải dữ liệu và lớp từ tập train\n",
    "    transform = transforms.Compose([Resize((224, 224)), ToTensor()])\n",
    "    train_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/train', transform=transform)\n",
    "    test_dataset = ImageFolder(root='/kaggle/input/dataset-split/dataset_split/test', transform=transform)\n",
    "    # Get class names\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    # 2. Tạo mô hình ViT và tải trạng thái từ checkpoint\n",
    "    # from utils.resnet18 import ResNet18\n",
    "\n",
    "    model = SEResNet18(\n",
    "        num_classes=len(class_names),\n",
    "    ).to(device)\n",
    "\n",
    "    folder_checkpoint = 'checkpoints_se_resnet18'  # Define the folder name\n",
    "    file_name = '/kaggle/input/resnet-train-acc/ResNet_train_acc/se_resnet18_epoch_94_acc_0.8560.pt' #best weight\n",
    "    file_checkpoint = os.path.join(folder_checkpoint, file_name)  # lay best weight\n",
    "    model.load_state_dict(torch.load(file_checkpoint, device))\n",
    "    print('Model loaded from checkpoint.')\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"output_resnet18\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 3. Dự đoán kết quả cho mỗi hình ảnh trong tập test\n",
    "    for image_path in tqdm(test_dataset.imgs, desc='Testing'):\n",
    "        img, predicted_label = predict_image(image_path[0], model, manual_transforms, class_names, device)\n",
    "        # plt.imshow(img)\n",
    "        # plt.title(f'Predicted: {predicted_label}')\n",
    "        # plt.show()\n",
    "\n",
    "        # Convert the tensor image back to a PIL image if necessary\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = transforms.ToPILImage()(img)\n",
    "\n",
    "        # Create a plot\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Set white background\n",
    "        fig.patch.set_facecolor('white')\n",
    "        ax.set_facecolor('white')\n",
    "\n",
    "        # Remove axis\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Add the predicted label as the title\n",
    "        ax.set_title(f'Predicted: {predicted_label}', fontsize=12, pad=10)\n",
    "\n",
    "        # Save the figure\n",
    "        image_basename = os.path.basename(image_path[0])\n",
    "        image_name, image_ext = os.path.splitext(image_basename)\n",
    "        output_image_path = os.path.join(output_dir, f\"{image_name}_pred_{predicted_label}.png\")\n",
    "\n",
    "        plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_model()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6740707,
     "sourceId": 10852843,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6760943,
     "sourceId": 10880967,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1658.698823,
   "end_time": "2025-02-28T13:40:17.164337",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-28T13:12:38.465514",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
